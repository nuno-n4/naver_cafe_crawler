import os
import time
import re
import requests
import random
from celery import Celery, Task
from flask_socketio import SocketIO
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime

# Celery ì•± ì„¤ì • (app.pyì™€ ë™ì¼í•˜ê²Œ)
celery_app = Celery('tasks', broker='redis://localhost:6379/0', backend='redis://localhost:6379/0')

# SocketIO í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ì„œë²„ë¡œ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ê¸° ìœ„í•¨)
socketio_client = SocketIO(message_queue='redis://')

def sanitize_filename(filename):
    """íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    return re.sub(r'[\/*?:"<>|]', "", filename).strip()

class ScrapeTask(Task):
    """ì§„í–‰ ìƒí™©ì„ ì—…ë°ì´íŠ¸ í•  ìˆ˜ ìˆëŠ” ì»¤ìŠ¤í…€ Task í´ë˜ìŠ¤"""
    def send_progress(self, message):
        """ì›¹í˜ì´ì§€ë¡œ ì§„í–‰ ìƒí™© ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ëŠ” í•¨ìˆ˜"""
        socketio_client.emit('progress_update', {'message': message})
        print(message) # Celery ì‘ì—…ì í„°ë¯¸ë„ì—ë„ ë¡œê·¸ ì¶œë ¥
        socketio_client.sleep(0.1)

def _perform_login(driver, task_instance):
    """ì‚¬ìš©ì ìˆ˜ë™ ë¡œê·¸ì¸ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    task_instance.send_progress("ë„¤ì´ë²„ ë¡œê·¸ì¸ì„ ìœ„í•´ ë¸Œë¼ìš°ì €ë¥¼ ì—½ë‹ˆë‹¤.")
    task_instance.send_progress("60ì´ˆ ë‚´ì— ì§ì ‘ ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”. ì™„ë£Œí•˜ë©´ ìë™ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.")
    driver.get('https://nid.naver.com/nidlogin.login')
    try:
        # ë¡œê·¸ì¸ ì„±ê³µ ì‹œ ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ì˜ íŠ¹ì • ìš”ì†Œê°€ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦¼
        WebDriverWait(driver, 60).until(
            EC.presence_of_element_located((By.ID, "account"))
        )
        task_instance.send_progress("ë¡œê·¸ì¸ ì„±ê³µì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
        return True
    except TimeoutException:
        task_instance.send_progress("[ì˜¤ë¥˜] 60ì´ˆ ë‚´ì— ë¡œê·¸ì¸ì´ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False

def _scrape_article_detail(driver, task_instance, article_url, download_dir):
    """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚´ìš©ê³¼ ì´ë¯¸ì§€ë¥¼ ìŠ¤í¬ë©í•˜ëŠ” í•¨ìˆ˜"""
    try:
        driver.get(article_url)
        
        # cafe_main iframeìœ¼ë¡œ ì „í™˜
        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "cafe_main")))

        title_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "h3.title_text"))
        )
        title = title_element.text
        sanitized_title = sanitize_filename(title)
        task_instance.send_progress(f"ê²Œì‹œê¸€ ìŠ¤í¬ë© ì¤‘: {title}")

        content = driver.find_element(By.CSS_SELECTOR, 'div.se-main-container').text

        # ê²Œì‹œê¸€ ë‚´ìš© ì €ì¥
        post_dir = os.path.join(download_dir, sanitized_title)
        os.makedirs(post_dir, exist_ok=True)
        with open(os.path.join(post_dir, 'content.txt'), 'w', encoding='utf-8') as f:
            f.write(content)

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        images = driver.find_elements(By.CSS_SELECTOR, 'img.se-image-resource')
        task_instance.send_progress(f"{len(images)}ê°œì˜ ì´ë¯¸ì§€ ë°œê²¬.")
        for i, img in enumerate(images):
            img_url = img.get_attribute('src')
            if img_url and not img_url.startswith('data:'): # base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ëŠ” ì œì™¸
                try:
                    img_data = requests.get(img_url, headers={'Referer': driver.current_url}).content
                    with open(os.path.join(post_dir, f'image_{i+1}.jpg'), 'wb') as f:
                        f.write(img_data)
                except Exception as e:
                    task_instance.send_progress(f"[ê²½ê³ ] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_url}, ì˜¤ë¥˜: {e}")
        
        driver.switch_to.default_content()
        return True

    except Exception as e:
        task_instance.send_progress(f"[ì˜¤ë¥˜] ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© ìŠ¤í¬ë© ì‹¤íŒ¨: {article_url}, ì˜¤ë¥˜: {e}")
        driver.switch_to.default_content() # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ iframeì—ì„œ ë¹ ì ¸ë‚˜ì˜¤ë„ë¡ ë³´ì¥
        return False


@celery_app.task(bind=True, base=ScrapeTask)
def run_cafe_scrape(self, cafe_id, start_date_str, end_date_str):
    """ë©”ì¸ í¬ë¡¤ë§ ë¡œì§ì„ ë‹´ê³  ìˆëŠ” Celery ì‘ì—…"""
    self.send_progress("í¬ë¡¤ë§ ì´ˆê¸°í™” ì¤‘...")
    
    start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()
    end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()
    
    download_dir = os.path.join(os.getcwd(), "downloads", sanitize_filename(cafe_id))
    os.makedirs(download_dir, exist_ok=True)

    service = Service(ChromeDriverManager().install())
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(service=service, options=options)
    driver.implicitly_wait(5)

    try:
        if not _perform_login(driver, self):
            raise Exception("ë¡œê·¸ì¸ ì‹¤íŒ¨")

        self.send_progress(f"ì¹´í˜(ID: {cafe_id}) í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        self.send_progress(f"ì¶”ì¶œ ê¸°ê°„: {start_date_str} ~ {end_date_str}")

        page = 1
        total_posts_scraped = 0
        stop_scraping = False

        while not stop_scraping:
            self.send_progress(f"--- {page} í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘ ---")
            board_url = f"https://cafe.naver.com/ca-fe/cafes/{cafe_id}/menus/0?page={page}"
            driver.get(board_url)
            
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "cafe_main")))

            articles = driver.find_elements(By.css_selector, 'div.article-board[class*="type-L"] > table > tbody > tr')
            if not articles:
                self.send_progress("ê²Œì‹œê¸€ì´ ë” ì´ìƒ ì—†ê±°ë‚˜, í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            article_links = []
            for article in articles:
                try:
                    date_element = article.find_element(By.css_selector, 'td.td_date')
                    post_date_str = date_element.text
                    
                    if ':' in post_date_str:
                        post_date = datetime.now().date()
                    else:
                        post_date = datetime.strptime(post_date_str, '%Y.%m.%d.').date()

                    if post_date > end_date:
                        continue
                    if post_date < start_date:
                        stop_scraping = True
                        break
                    
                    link_element = article.find_element(By.css_selector, 'a.article')
                    article_links.append(link_element.get_attribute('href'))

                except NoSuchElementException:
                    self.send_progress("[ì •ë³´] ë‚ ì§œ ì •ë³´ê°€ ì—†ëŠ” í–‰(ê³µì§€ ë“±)ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
            
            if stop_scraping:
                self.send_progress(f"ê²Œì‹œê¸€ ë‚ ì§œê°€ ì‹œì‘ì¼({start_date_str})ë³´ë‹¤ ì´ì „ì´ë¯€ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            self.send_progress(f"{len(article_links)}ê°œì˜ ê²Œì‹œê¸€ì„ ë°œê²¬í•˜ì—¬ ìŠ¤í¬ë©ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            for link in article_links:
                if _scrape_article_detail(driver, self, link, download_dir):
                    total_posts_scraped += 1
                time.sleep(random.uniform(1, 3)) # ë‹¤ìŒ ê²Œì‹œê¸€ ìŠ¤í¬ë© ì „ ì§€ì—°

            page += 1
            driver.switch_to.default_content()

    except Exception as e:
        self.send_progress(f"[ì¹˜ëª…ì  ì˜¤ë¥˜] í¬ë¡¤ë§ ì¤‘ë‹¨: {e}")
    finally:
        driver.quit()
        self.send_progress(f"í¬ë¡¤ë§ ì™„ë£Œ! ì´ {total_posts_scraped}ê°œì˜ ê²Œì‹œê¸€ì„ ìŠ¤í¬ë©í–ˆìŠµë‹ˆë‹¤. ğŸ‰")

    return f"ì´ {total_posts_scraped}ê°œ ê²Œì‹œê¸€ ì²˜ë¦¬ ì™„ë£Œ."